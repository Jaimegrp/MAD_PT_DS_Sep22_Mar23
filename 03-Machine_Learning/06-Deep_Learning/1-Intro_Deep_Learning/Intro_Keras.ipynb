{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPor defecto, keras no tira de GPU\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras (NO NECESARIO YA INTEGRADO EN TENSORFLOW)\n",
    "'''\n",
    "Por defecto, keras no tira de GPU\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (these are NumPy arrays). Aplano a una dimension cada imagen.\n",
    "# Escalamos ya que vamos a usar gradient descent, y le afecta mucho la escala de las features.\n",
    "# Ejecutar esta celda solo una vez. Sino reescalará\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Reserve 10,000 samples for validation. Entraran dentro del modelo para validar. No es validacion cruzada\n",
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "print(X_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINICION/CONSTRUCCION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax.  \n",
    "\n",
    "Es decir vamos a volver a montar esta arquitectura:  \n",
    "<img src=\"./img/mlp_clasification.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "Un poco más sobre la activación softmax:    \n",
    "\n",
    "Fórmula:  \n",
    "<img src=\"./img/softmax_function.png\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "Función de transferencia:  \n",
    "<img src=\"./img/softmax_activation.png\" alt=\"drawing\" width=\"150\"/>\n",
    "\n",
    "Ejemplo de funcionamiento:  \n",
    "<img src=\"./img/softmax_example.png\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una manera de declarar la red neuronal\n",
    "\n",
    "# Siempre hay que declarar la capa sequential para empezar a declarar la red\n",
    "# Se trata de la API sequential\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Flatten, aplana en un unico vector. Y especificamos el tamaño de la entrada\n",
    "# Es como si hiciese un .reshape(-1, 28*28)\n",
    "# \"kernel_initializer\" o \"bias_initializer\" No lo usamos pero seria para inicializar los pesos de otra manera\n",
    "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "\n",
    "# Capas de la red. Dense es la capa de neuronas. Necesitamos numero y activacion\n",
    "model.add(keras.layers.Dense(units = 300, # Numero de neuronas de la capa\n",
    "                             activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(100,\n",
    "                             activation='relu'))\n",
    "\n",
    "# Capa de salida, con tamaño del número de clases\n",
    "# Suele ir aqui un softmax. Para multiclase guay. Si es binaria -> sigmoide\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "]\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y una forma mediante encademaniento de funciones (usando lo que se denomina la Functional API)\n",
    "input_layer = keras.layers.Input(shape = (28,28))\n",
    "flatten_layer = keras.layers.Flatten()(input_layer)\n",
    "hidden_1 = keras.layers.Dense(300, activation = \"relu\")(flatten_layer)\n",
    "hidden_2 = keras.layers.Dense(100, activation = \"relu\")(hidden_1)\n",
    "output = keras.layers.Dense(10, activation = \"softmax\")(hidden_2)\n",
    "model = keras.Model(inputs = [input_layer], outputs = [output])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La functional API me va a permitir construir redes como estas:\n",
    "\n",
    "<img src=\"./img/otras_arquitecturas.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.reshaping.flatten.Flatten object at 0x0000027767A038E0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x277640c5e40>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x27767a038e0>,\n",
       " <keras.layers.core.dense.Dense at 0x2776799d900>,\n",
       " <keras.layers.core.dense.Dense at 0x2776799d120>,\n",
       " <keras.layers.core.dense.Dense at 0x2776799d8d0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.layers[1])\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[2]\n",
    "weights, biases = hidden1.get_weights()\n",
    "\n",
    "# 784 features (pixeles de las imagenes) x 300 neuronas\n",
    "# Los pesos están inicializados aleatoriamente\n",
    "weights.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializadores:  \n",
    "- Los pesos inicializados a cero -> No aprendizaje\n",
    "- Desde siempre se inicializan \"aleatoriamente\", pero no sólo de forma uniforme (todos los valores con la misma probabilidad), sino que se emplean diferentes distribuciones de probabilidad con parámetros que dependen del número de entradas y salidas de la capa. El objetivo esintentar que las varianzas de las entradas sean similares a las de las salidas y evitar el problema del gradiente que se desvanece (\"Vanishing Gradient\" problem):  \n",
    "    *   Glorot inizialization (por defecto la de Keras, con función uniforme de distribución) -> Para cuando tienes funciones de activación (ninguna, tanh, sigmoid, softmax, aunque también se usa por defecto :-) para casi todo) [Xavier Glorot & Yoshua Bengio]\n",
    "    *   He inizialization, -> Para cuando tienes ReLU, Leaky ReLU, ELU, GELU, Swish, Mish [He Kaiming et al.]\n",
    "    *   LeCunn inizialization -> Para cuando tienes SELU [Jean LeCunn]\n",
    "- Es un hiperparámetro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se especifica la configuración del entrenamiento (optimizador, pérdida, métricas):\n",
    "model.compile(\n",
    "    # Stocastic gradient descent. El algoritmo para minimizar la loss function\n",
    "    # El stocastic va haciendo muestreo en cada evaluacion, no usa todo el dataset\n",
    "    # Podemos modificar el learning rate(0.01 por defecto) mediante el parametro lr\n",
    "    optimizer=keras.optimizers.SGD(),  # Optimizer, con parámetros por defecto\n",
    "    \n",
    "    \n",
    "    # Loss function to minimize\n",
    "    # sparse_categorical_crossentropy cuando tenemos un label en nuna columna\n",
    "    # Si lo tuviesemos en varias tipo dummy, cogeriamos categorical_crossentropy\n",
    "    # binary_crossentropy si es una neurona, clasi binario\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente (... casi, los parámetros del optimizador serán los que tenga por defecto)\n",
    "model.compile(optimizer=\"sgd\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "# La primera hidden layer tiene 784 entradas x 300 salidas\n",
    "# Son los 235500 params = 783x300 + 300 (bias)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAPAS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vayamos construyendo nuestra __lista de capas__ (para guardar en el \"Toolbox\"):\n",
    "\n",
    "__Entrenables__:  \n",
    "__* Dense__ -> Capa completamente conectada a las neuronas de la capa anterior y a la posterior  \n",
    "    Hiperparámetros asociados:     \n",
    "        * units: Number of neurons, dimensionality of the output space  \n",
    "        * activation: Activation function to use. If you don't specify anything, no activation is applied  \n",
    "        * kernel_initializer: Initializer for the kernel weights matrix.  \n",
    "        * bias_initializer: Initializer for the bias vector. (Suelen inicializarse a cero)\n",
    "        * Kernel_regularizar: Los clásicos (L1,L2,...)\n",
    " \n",
    "__Funcionales__:       \n",
    "__* Input__ -> Capa para definir la forma de la entrada (shape), que se puede pasar como input_shape\n",
    "__* Flatten__ -> Capa que aplana (convierte su entrada en un vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras funciones de activación interesantes: SELU (1.67*ELU) y Swish (también SiLU, o Sigmoid linear unit)... No entrar en pánico, vais a usar ReLU, Softmax y no-activation, y en algunos casos (quizás): sigmoid, tanh y las (x)LU (SELU, Siwsh,etc)\n",
    "\n",
    "<img src=\"./img/activation_functions.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIMIZADORES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y también vamos completando lista de hiperparámetros, estos asociados al \"Optimizador\"/\"Modelo\":  \n",
    "Tipo de optimizador:  \n",
    "* __SGD__, Gradient descent \"genérico\" (puedes añadirle \"momento\", es decir que a la hora de descontar el gradiente tenga en cuenta el vector medio de gradientes pasados)\n",
    "  \n",
    "\n",
    "* __Adagrad__, Hace gradient descent pero ajusta el gradiente para compensar las componentes de mayor valor numérico (es como evitar irse por las pendientes más inclinadas)... Es decir evita irse a mínimos locales al precio de enlentecer el entrenamiento.    \n",
    "\n",
    "* __RMSprop__, Versión de AdaGrad, pero considera principalmente los últimos valores del gradiente. Es decir, busca lo bueno de Adagrad reduciendo sus peligros.    \n",
    "\n",
    "* __Adam__, _Adaptative Moment Estimation_, combina RMSProp y el uso de momento. Es el rey actual (junto con sus versiones) para grandes cantidades de datos.    \n",
    "\n",
    "* __AdamW, Nadam, AdaMax__, variantes del anterior. \n",
    "\n",
    "Comparativa, donde * es malo y *** bueno (extraído del \"Hands-on Machine Learning with....\" de Aurelien Geron, 3a Edicion)\n",
    "\n",
    "<img src=\"./img/Comparativa-optimizadores.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "Keras también permite:\n",
    "__Adadelta__ _(variante de Adagrad)_, __Adafactor__ y __Ftrl__\n",
    "\n",
    "      \n",
    "Hiperparámetros Genéricos:\n",
    "Learning Rate: Coeficiente aplicado al descenso de gradiente, como en otros modelos que ya hemos visto\n",
    "Asociados al Gradient Clipping: clipnorm, clipvalue, global_clipnorm\n",
    "\n",
    "Cada optimizador además puede tener sus propios hiperparámetros (ver: https://keras.io/api/optimizers/)\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuciones de pérdida y métricas\n",
    "__Función de perdida__: La función a minimizar durante el entrenamiento (son las mismas que en otros modelos no Deep)  \n",
    "- Clasificación: En clases Keras -> __BinaryCrossEntropy, CategoricalCrossEntropy, SparseCategoricalCrossEntropy__  \n",
    "- Regresión: En clases Keras -> __MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, CosineSimilarity__   \n",
    "\n",
    "__Métricas__:  \n",
    "- Regresión: __MAE, MSE, MAPE__ :-)  \n",
    "- Clasificación: __Accuracy, Precision, Recall, f1, AuRoC__  \n",
    "\n",
    "¿Cuál es la diferencia entre Categorical y Sparse? ¿Por qué las funciones de pérdida son diferentes a las métricas en Clasificación? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El __batch_size__ es la cantidad de muestras que utiliza el SGD, y las __epochs__ son las iteraciones que realiza en el entrenamiento. (Son hiperparámetros de entrenamiento)    \n",
    "\n",
    "En una epoch se entrenan tantos batches como sea necesario para recorrer todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/15\n",
      "227/782 [=======>......................] - ETA: 0s - loss: 0.1334 - accuracy: 0.9618"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Entrenamos el modelo con un batch_size de 64 imágenes por cada iteración, 10 epochs y especificando cuál es el conjunto de validación.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFit model on training data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      4\u001b[0m     X_train,\n\u001b[0;32m      5\u001b[0m     y_train,\n\u001b[0;32m      6\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, \u001b[39m# numero de muestras empleadas en el entrenamiento de SGD\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, \u001b[39m# 1 por defecto. Insuficiente. Numero de vueltas del backpropagation\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39m# We pass some validation for\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m     \u001b[39m# monitoring validation loss and metrics\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m     \u001b[39m# at the end of each epoch\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39m# En vez de validation data podemos usar el argumento validation_split=0.1\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_val, y_val)\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mEn el entreanamiento solo hay que fijarse que el loss va para abajo, es bueno.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mSi vemos que ya no baja mas, no serán necesarias tantas epochs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mEl loss que muestra es el categoricalcrossentropy\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m'''\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\glezr\\Deep_Learning\\BootCamp\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Entrenamos el modelo con un batch_size de 64 imágenes por cada iteración, 10 epochs y especificando cuál es el conjunto de validación.\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64, # numero de muestras empleadas en el entrenamiento de SGD\n",
    "    epochs=15, # 1 por defecto. Insuficiente. Numero de vueltas del backpropagation\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    # En vez de validation data podemos usar el argumento validation_split=0.1\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "'''\n",
    "En el entreanamiento solo hay que fijarse que el loss va para abajo, es bueno.\n",
    "Si vemos que ya no baja mas, no serán necesarias tantas epochs.\n",
    "Imprimera tantas lineas como epochs hayamos puesto\n",
    "\n",
    "Tampoco usamos el class_weight, que le da más peso a las clases con pocas muestras\n",
    "Util para datasets desbalanceados.\n",
    "\n",
    "El loss que muestra es el categoricalcrossentropy\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1239 - accuracy: 0.9649 - val_loss: 0.1356 - val_accuracy: 0.9617\n",
      "Epoch 2/15\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1183 - accuracy: 0.9670 - val_loss: 0.1297 - val_accuracy: 0.9649\n",
      "Epoch 3/15\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1125 - accuracy: 0.9678 - val_loss: 0.1248 - val_accuracy: 0.9664\n",
      "Epoch 4/15\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1078 - accuracy: 0.9698 - val_loss: 0.1218 - val_accuracy: 0.9670\n",
      "Epoch 5/15\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1030 - accuracy: 0.9713 - val_loss: 0.1181 - val_accuracy: 0.9688\n",
      "Epoch 6/15\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0988 - accuracy: 0.9724 - val_loss: 0.1154 - val_accuracy: 0.9689\n",
      "Epoch 7/15\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0945 - accuracy: 0.9733 - val_loss: 0.1134 - val_accuracy: 0.9686\n",
      "Epoch 8/15\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0907 - accuracy: 0.9750 - val_loss: 0.1122 - val_accuracy: 0.9689\n",
      "Epoch 9/15\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0873 - accuracy: 0.9759 - val_loss: 0.1127 - val_accuracy: 0.9693\n",
      "Epoch 10/15\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0839 - accuracy: 0.9766 - val_loss: 0.1086 - val_accuracy: 0.9687\n",
      "Epoch 11/15\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0808 - accuracy: 0.9774 - val_loss: 0.1052 - val_accuracy: 0.9713\n",
      "Epoch 12/15\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0777 - accuracy: 0.9785 - val_loss: 0.1047 - val_accuracy: 0.9707\n",
      "Epoch 13/15\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.9797 - val_loss: 0.1042 - val_accuracy: 0.9688\n",
      "Epoch 14/15\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0721 - accuracy: 0.9807 - val_loss: 0.1049 - val_accuracy: 0.9693\n",
      "Epoch 15/15\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0697 - accuracy: 0.9813 - val_loss: 0.1005 - val_accuracy: 0.9717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2776799d060>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=15,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 1, 'epochs': 15, 'steps': 782}\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.8653184175491333,\n",
       "  0.37718307971954346,\n",
       "  0.31273114681243896,\n",
       "  0.27795034646987915,\n",
       "  0.2524315118789673,\n",
       "  0.23230259120464325,\n",
       "  0.21500366926193237,\n",
       "  0.1999616175889969,\n",
       "  0.18660804629325867,\n",
       "  0.17514637112617493,\n",
       "  0.16470976173877716,\n",
       "  0.15538306534290314,\n",
       "  0.14697298407554626,\n",
       "  0.13917842507362366,\n",
       "  0.13235484063625336],\n",
       " 'accuracy': [0.791159987449646,\n",
       "  0.8963199853897095,\n",
       "  0.9111400246620178,\n",
       "  0.920520007610321,\n",
       "  0.9276400208473206,\n",
       "  0.9338200092315674,\n",
       "  0.9381999969482422,\n",
       "  0.9426400065422058,\n",
       "  0.9466000199317932,\n",
       "  0.9501199722290039,\n",
       "  0.9525600075721741,\n",
       "  0.9553599953651428,\n",
       "  0.9578999876976013,\n",
       "  0.9602400064468384,\n",
       "  0.9619799852371216],\n",
       " 'val_loss': [0.40332141518592834,\n",
       "  0.30681177973747253,\n",
       "  0.2741599977016449,\n",
       "  0.25080054998397827,\n",
       "  0.2291218787431717,\n",
       "  0.21533620357513428,\n",
       "  0.19957205653190613,\n",
       "  0.19031009078025818,\n",
       "  0.17903535068035126,\n",
       "  0.16796830296516418,\n",
       "  0.1651764214038849,\n",
       "  0.15260769426822662,\n",
       "  0.14798428118228912,\n",
       "  0.14240317046642303,\n",
       "  0.1459202766418457],\n",
       " 'val_accuracy': [0.8978000283241272,\n",
       "  0.911899983882904,\n",
       "  0.9210000038146973,\n",
       "  0.9265000224113464,\n",
       "  0.9343000054359436,\n",
       "  0.9377999901771545,\n",
       "  0.9441999793052673,\n",
       "  0.9472000002861023,\n",
       "  0.9506000280380249,\n",
       "  0.9537000060081482,\n",
       "  0.9550999999046326,\n",
       "  0.9577000141143799,\n",
       "  0.9595000147819519,\n",
       "  0.9610999822616577,\n",
       "  0.9602000117301941]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(history.params)\n",
    "print(history.epoch)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Podemos ver como evoluciona el entrenamiento, en funcion de los epochs\n",
    "# Validacion y training estan muy cerca, no hay overfitting!\n",
    "# Todavia no ha acabado de coverger ya que el loss en validacion sigue bajando,\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0960 - sparse_categorical_accuracy: 0.9699\n",
      "test loss, test acc: [0.09595837444067001, 0.9699000120162964]\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el \"score\" a partir del conjunto de test\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "\n",
    "# Metodo evaluate para que nos de el error vs las metricas elegidas en la funcion compile\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions shape: (1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.001, 0.001, 0.   , 0.   , 0.   , 0.998, 0.   ,\n",
       "        0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Ojo aqui viene slicing xq presupone que le entran varios inputs\n",
    "Nos da las probabilidades de pertenecer a una clase u otra.\n",
    "'''\n",
    "predictions = model.predict(X_test[:1]).round(3)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona.\n",
    "     \n",
    "Vamos a configurar una red como esta:  \n",
    "<img src=\"./img/mlp_regresion.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7415 - root_mean_squared_error: 0.8611 - val_loss: 0.4936 - val_root_mean_squared_error: 0.7026\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 967us/step - loss: 0.4566 - root_mean_squared_error: 0.6757 - val_loss: 0.4299 - val_root_mean_squared_error: 0.6557\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 976us/step - loss: 0.4172 - root_mean_squared_error: 0.6459 - val_loss: 0.8692 - val_root_mean_squared_error: 0.9323\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 857us/step - loss: 0.4028 - root_mean_squared_error: 0.6347 - val_loss: 0.5007 - val_root_mean_squared_error: 0.7076\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 884us/step - loss: 0.3969 - root_mean_squared_error: 0.6300 - val_loss: 0.4306 - val_root_mean_squared_error: 0.6562\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3849 - root_mean_squared_error: 0.6204 - val_loss: 0.5173 - val_root_mean_squared_error: 0.7192\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 884us/step - loss: 0.3812 - root_mean_squared_error: 0.6174 - val_loss: 0.3823 - val_root_mean_squared_error: 0.6183\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.3772 - root_mean_squared_error: 0.6141 - val_loss: 0.5644 - val_root_mean_squared_error: 0.7513\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 871us/step - loss: 0.3747 - root_mean_squared_error: 0.6121 - val_loss: 0.4190 - val_root_mean_squared_error: 0.6473\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 880us/step - loss: 0.3698 - root_mean_squared_error: 0.6082 - val_loss: 0.4448 - val_root_mean_squared_error: 0.6669\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 892us/step - loss: 0.3673 - root_mean_squared_error: 0.6060 - val_loss: 0.6610 - val_root_mean_squared_error: 0.8130\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 883us/step - loss: 0.3656 - root_mean_squared_error: 0.6047 - val_loss: 0.3732 - val_root_mean_squared_error: 0.6109\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 912us/step - loss: 0.3634 - root_mean_squared_error: 0.6029 - val_loss: 0.6668 - val_root_mean_squared_error: 0.8166\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.3611 - root_mean_squared_error: 0.6009 - val_loss: 0.4565 - val_root_mean_squared_error: 0.6756\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 895us/step - loss: 0.3603 - root_mean_squared_error: 0.6003 - val_loss: 1.2693 - val_root_mean_squared_error: 1.1266\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 871us/step - loss: 0.3586 - root_mean_squared_error: 0.5988 - val_loss: 0.4583 - val_root_mean_squared_error: 0.6770\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 897us/step - loss: 0.3543 - root_mean_squared_error: 0.5952 - val_loss: 0.4933 - val_root_mean_squared_error: 0.7023\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3542 - root_mean_squared_error: 0.5951 - val_loss: 0.5655 - val_root_mean_squared_error: 0.7520\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.3516 - root_mean_squared_error: 0.5930 - val_loss: 0.3697 - val_root_mean_squared_error: 0.6080\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 871us/step - loss: 0.3507 - root_mean_squared_error: 0.5922 - val_loss: 0.3604 - val_root_mean_squared_error: 0.6003\n",
      "162/162 [==============================] - 0s 729us/step - loss: 0.3422 - root_mean_squared_error: 0.5850\n",
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    \n",
    "    # No hace falta capa de flatten. No hay que aplanar ninguna imagen\n",
    "    keras.layers.Dense(30, activation=\"relu\",\n",
    "                       input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1) # una unica neurona de salida\n",
    "    # Sin fun de activa. ReLu no iria mal si el output es positivo. Sigmoide si esta acotado.\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics = [\"RootMeanSquaredError\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.34220629930496216, 0.5849840044975281]\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE:\",mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.8024 - root_mean_squared_error: 0.8958 - mean_absolute_error: 0.6400 - val_loss: 3.0219 - val_root_mean_squared_error: 1.7384 - val_mean_absolute_error: 0.6083\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.2714 - root_mean_squared_error: 1.1276 - mean_absolute_error: 0.6183 - val_loss: 3.0771 - val_root_mean_squared_error: 1.7542 - val_mean_absolute_error: 0.6121\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 924us/step - loss: 0.8020 - root_mean_squared_error: 0.8955 - mean_absolute_error: 0.5469 - val_loss: 5.6667 - val_root_mean_squared_error: 2.3805 - val_mean_absolute_error: 0.5936\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 6.9302 - root_mean_squared_error: 2.6325 - mean_absolute_error: 0.6748 - val_loss: 0.5193 - val_root_mean_squared_error: 0.7206 - val_mean_absolute_error: 0.5073\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.4983 - root_mean_squared_error: 0.7059 - mean_absolute_error: 0.4944 - val_loss: 0.4766 - val_root_mean_squared_error: 0.6904 - val_mean_absolute_error: 0.4801\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 969us/step - loss: 0.4576 - root_mean_squared_error: 0.6765 - mean_absolute_error: 0.4775 - val_loss: 0.4532 - val_root_mean_squared_error: 0.6732 - val_mean_absolute_error: 0.4669\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4328 - root_mean_squared_error: 0.6579 - mean_absolute_error: 0.4648 - val_loss: 0.4378 - val_root_mean_squared_error: 0.6617 - val_mean_absolute_error: 0.4623\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 962us/step - loss: 0.4140 - root_mean_squared_error: 0.6434 - mean_absolute_error: 0.4550 - val_loss: 0.4300 - val_root_mean_squared_error: 0.6557 - val_mean_absolute_error: 0.4524\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4026 - root_mean_squared_error: 0.6345 - mean_absolute_error: 0.4476 - val_loss: 0.4196 - val_root_mean_squared_error: 0.6478 - val_mean_absolute_error: 0.4486\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 952us/step - loss: 0.3967 - root_mean_squared_error: 0.6299 - mean_absolute_error: 0.4457 - val_loss: 0.4184 - val_root_mean_squared_error: 0.6468 - val_mean_absolute_error: 0.4442\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 967us/step - loss: 0.3883 - root_mean_squared_error: 0.6232 - mean_absolute_error: 0.4409 - val_loss: 0.4100 - val_root_mean_squared_error: 0.6403 - val_mean_absolute_error: 0.4361\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.3830 - root_mean_squared_error: 0.6189 - mean_absolute_error: 0.4372 - val_loss: 0.4077 - val_root_mean_squared_error: 0.6385 - val_mean_absolute_error: 0.4408\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3758 - root_mean_squared_error: 0.6130 - mean_absolute_error: 0.4335 - val_loss: 0.4036 - val_root_mean_squared_error: 0.6353 - val_mean_absolute_error: 0.4380\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 956us/step - loss: 0.3708 - root_mean_squared_error: 0.6090 - mean_absolute_error: 0.4302 - val_loss: 0.4056 - val_root_mean_squared_error: 0.6369 - val_mean_absolute_error: 0.4272\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3677 - root_mean_squared_error: 0.6064 - mean_absolute_error: 0.4276 - val_loss: 0.3979 - val_root_mean_squared_error: 0.6308 - val_mean_absolute_error: 0.4338\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 948us/step - loss: 0.3633 - root_mean_squared_error: 0.6028 - mean_absolute_error: 0.4246 - val_loss: 0.3950 - val_root_mean_squared_error: 0.6285 - val_mean_absolute_error: 0.4363\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.3604 - root_mean_squared_error: 0.6003 - mean_absolute_error: 0.4225 - val_loss: 0.3873 - val_root_mean_squared_error: 0.6224 - val_mean_absolute_error: 0.4224\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 908us/step - loss: 0.3589 - root_mean_squared_error: 0.5991 - mean_absolute_error: 0.4208 - val_loss: 0.3896 - val_root_mean_squared_error: 0.6242 - val_mean_absolute_error: 0.4238\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 907us/step - loss: 0.3563 - root_mean_squared_error: 0.5969 - mean_absolute_error: 0.4188 - val_loss: 0.3854 - val_root_mean_squared_error: 0.6208 - val_mean_absolute_error: 0.4193\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 923us/step - loss: 0.3505 - root_mean_squared_error: 0.5920 - mean_absolute_error: 0.4154 - val_loss: 0.3893 - val_root_mean_squared_error: 0.6239 - val_mean_absolute_error: 0.4303\n",
      "162/162 [==============================] - 0s 827us/step - loss: 0.3485 - root_mean_squared_error: 0.5904 - mean_absolute_error: 0.4204\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    }
   ],
   "source": [
    "#Otra forma pure-Keras:\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "\n",
    "norm_layer = keras.layers.Normalization(input_shape = X_train.shape[1:]) # Es una Standardization\n",
    "model = keras.models.Sequential([\n",
    "    \n",
    "    # No hace falta capa de flatten. No hay que aplanar ninguna imagen\n",
    "    norm_layer,\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1) # una unica neurona de salida\n",
    "    # Sin fun de activa. ReLu no iria mal si el output es positivo. Sigmoide si esta acotado.\n",
    "])\n",
    "optimizer = keras.optimizers.SGD()\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=optimizer, metrics = [\"RootMeanSquaredError\",\"MeanAbsoluteError\"])\n",
    "norm_layer.adapt(X_train)\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.348535418510437, 0.5903688669204712, 0.4204059839248657]\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE:\",mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nueva capa al toolbox:\n",
    "\n",
    "Funcionales:  \n",
    "__Normalize__: keras.layers.Normalization -> Nos hace la standardizacion de la entrada \n",
    "Hay que ejecutar el metodo Adapt antes de llamar al fit del modelo que incluya la capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma es emplear el formato TensorFlow. En este caso crea un directorio con varios ficheros que facilita el despliegue en algunas aplicaciones (ojo, que habría que llevar a producción todo el directorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k.save(\"my_model_tf_format\", save_format= \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo volvemos a cargar\n",
    "model_k= keras.models.load_model(\"my_model_tf_format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.3094\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.3074\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.3076\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3089\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3057\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 882us/step - loss: 0.3059\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 882us/step - loss: 0.3051\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3048\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.3067\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 882us/step - loss: 0.3037\n",
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3057 - val_loss: 0.3335\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3064 - val_loss: 0.3203\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3023 - val_loss: 0.3221\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3090 - val_loss: 0.3169\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3090 - val_loss: 0.3200\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3050 - val_loss: 0.3153\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3035 - val_loss: 0.3141\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 965us/step - loss: 0.3038 - val_loss: 0.3389\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3202 - val_loss: 0.3169\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.3433\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3184 - val_loss: 0.3198\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3103 - val_loss: 0.3228\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3163 - val_loss: 0.3201\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3062 - val_loss: 0.3202\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 921us/step - loss: 0.3065 - val_loss: 0.3236\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 894us/step - loss: 0.3046 - val_loss: 0.3197\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3046 - val_loss: 0.3230\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3051 - val_loss: 0.3193\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3038 - val_loss: 0.3147\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3031 - val_loss: 0.3164\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3085 - val_loss: 0.3205\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3073 - val_loss: 0.3173\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3063 - val_loss: 0.3162\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.3055 - val_loss: 0.3269\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3062 - val_loss: 0.3199\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 886us/step - loss: 0.3059 - val_loss: 0.3199\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 841us/step - loss: 0.3065 - val_loss: 0.3309\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 775us/step - loss: 0.3048 - val_loss: 0.3232\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "10 esta bien. Lo pondemos a 5 para el ejercicio\n",
    "¿Qué considera como dejar de mejorar? parametros min_delta y baseline\n",
    "'''\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, \n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros y tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guía \"casera\":\n",
    "\n",
    "1- Recetas (adaptada de \"Hands-on...\")  PARA MLPs!!!!    \n",
    "    * Capas:   \n",
    "        - Empezar con una capa oculta e ir añadiendo (dependiendo de la complejidad del problema, probar wide & deep)\n",
    "        - Si pocas features -> más neuronas  (aumentar la combinación de features) (num_features < 100) [Orientativo]  \n",
    "        - Si muchas features  -> menos neuronas (proyección tipo PCA) (num_features > 1000) [Orientativo] e ir aumentando en capas sucesivas\n",
    "        - O empezar con muchas (doble de tus features e ir \"estrechando los pantalones\")  \n",
    "        - Construcción en prisma o pirámide (para empezar)  \n",
    "        - Inicialización: Empezar con Glorot, cambiar a He  \n",
    "        - Activación: ReLU salvo la última, si muchas capas probar -> SELU o Swish (con el inicializador a LeCunn) \n",
    "    * Optimizadores:   \n",
    "        - Si muchos datos*features -> Adam o AdamW con sus valores por defecto  \n",
    "        - Si no, SGD con Nesterov activado, y momento a 0.9  \n",
    "        - Learning rate -> 0.001-0.0001 para empezar e ir creciendo (learning-rate warm-up) (Si te atreves, buscar adaptative learning rate y optimizar con esto)  \n",
    "    * Entrenamiento:  \n",
    "        - Epoch, probar con pocas para ver duración -> Epochs altas y Callback de Early Stop activado  \n",
    "        - Batch_Size -> 32, si tienes muchos datos y una GPU a mano puedes subir mucho 64,128,256...\n",
    "    * Regularización (lo veremos):  \n",
    "        - Dropout al 0.25-0.5 (sin SELU)\n",
    "\n",
    "\n",
    "\n",
    "2- Pasos  \n",
    "    - Si overfitting -> Regularizar: Earlystopping, Dropout (lo veremos en la siguiente sección)  \n",
    "    - Comprobar underfitting -> Aumentar epochs, aumentar batch_size  \n",
    "    - Jugar con optimizador: learning rate (de pequeño a grande), tipo de optimizador   \n",
    "    - Jugar con número de capas (ojo al overfitting) y las funciones de activación y la inicialización de pesos  \n",
    "    - Jugar con el número de neuronas por capa (suele ser piramide o prisma, pero puedes jugar a expandir dimensiones)  \n",
    "    - Combinar los dos anteriores  \n",
    "\n",
    "3- Keras Tuner:\n",
    "    https://keras.io/guides/keras_tuner/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herramientas adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard\n",
    "Keras tiene implementado un dashboard para monitorizar las ejecuciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Crea este directorio\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "# Guarda una carpeta nueva con la fecha de la ejecucion\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3470 - root_mean_squared_error: 0.5890 - mean_absolute_error: 0.4128 - val_loss: 0.3871 - val_root_mean_squared_error: 0.6222 - val_mean_absolute_error: 0.4281\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3443 - root_mean_squared_error: 0.5867 - mean_absolute_error: 0.4114 - val_loss: 0.3928 - val_root_mean_squared_error: 0.6268 - val_mean_absolute_error: 0.4219\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.3420 - root_mean_squared_error: 0.5848 - mean_absolute_error: 0.4101 - val_loss: 0.3787 - val_root_mean_squared_error: 0.6154 - val_mean_absolute_error: 0.4237\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 0s 979us/step - loss: 0.3429 - root_mean_squared_error: 0.5856 - mean_absolute_error: 0.4089 - val_loss: 0.6850 - val_root_mean_squared_error: 0.8276 - val_mean_absolute_error: 0.5187\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 0s 936us/step - loss: 0.3764 - root_mean_squared_error: 0.6136 - mean_absolute_error: 0.4259 - val_loss: 0.4742 - val_root_mean_squared_error: 0.6886 - val_mean_absolute_error: 0.4383\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 0s 934us/step - loss: 0.3573 - root_mean_squared_error: 0.5977 - mean_absolute_error: 0.4169 - val_loss: 0.5347 - val_root_mean_squared_error: 0.7312 - val_mean_absolute_error: 0.4573\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3528 - root_mean_squared_error: 0.5939 - mean_absolute_error: 0.4137 - val_loss: 0.3997 - val_root_mean_squared_error: 0.6322 - val_mean_absolute_error: 0.4317\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.3434 - root_mean_squared_error: 0.5860 - mean_absolute_error: 0.4104 - val_loss: 0.3705 - val_root_mean_squared_error: 0.6087 - val_mean_absolute_error: 0.4145\n",
      "Epoch 9/50\n",
      "363/363 [==============================] - 0s 1000us/step - loss: 0.3350 - root_mean_squared_error: 0.5788 - mean_absolute_error: 0.4058 - val_loss: 0.3738 - val_root_mean_squared_error: 0.6114 - val_mean_absolute_error: 0.4208\n",
      "Epoch 10/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3326 - root_mean_squared_error: 0.5767 - mean_absolute_error: 0.4051 - val_loss: 0.3715 - val_root_mean_squared_error: 0.6095 - val_mean_absolute_error: 0.4170\n",
      "Epoch 11/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3301 - root_mean_squared_error: 0.5746 - mean_absolute_error: 0.4037 - val_loss: 0.3731 - val_root_mean_squared_error: 0.6108 - val_mean_absolute_error: 0.4133\n",
      "Epoch 12/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3287 - root_mean_squared_error: 0.5733 - mean_absolute_error: 0.4020 - val_loss: 0.3686 - val_root_mean_squared_error: 0.6072 - val_mean_absolute_error: 0.4123\n",
      "Epoch 13/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3305 - root_mean_squared_error: 0.5749 - mean_absolute_error: 0.4021 - val_loss: 0.3823 - val_root_mean_squared_error: 0.6183 - val_mean_absolute_error: 0.4130\n",
      "Epoch 14/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3321 - root_mean_squared_error: 0.5763 - mean_absolute_error: 0.4024 - val_loss: 0.3714 - val_root_mean_squared_error: 0.6094 - val_mean_absolute_error: 0.4126\n",
      "Epoch 15/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3281 - root_mean_squared_error: 0.5728 - mean_absolute_error: 0.4013 - val_loss: 0.3673 - val_root_mean_squared_error: 0.6061 - val_mean_absolute_error: 0.4123\n",
      "Epoch 16/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3256 - root_mean_squared_error: 0.5706 - mean_absolute_error: 0.3997 - val_loss: 0.3660 - val_root_mean_squared_error: 0.6050 - val_mean_absolute_error: 0.4145\n",
      "Epoch 17/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3245 - root_mean_squared_error: 0.5696 - mean_absolute_error: 0.3995 - val_loss: 0.3685 - val_root_mean_squared_error: 0.6071 - val_mean_absolute_error: 0.4100\n",
      "Epoch 18/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3324 - root_mean_squared_error: 0.5766 - mean_absolute_error: 0.4026 - val_loss: 0.3710 - val_root_mean_squared_error: 0.6091 - val_mean_absolute_error: 0.4128\n",
      "Epoch 19/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3301 - root_mean_squared_error: 0.5745 - mean_absolute_error: 0.4030 - val_loss: 0.3645 - val_root_mean_squared_error: 0.6037 - val_mean_absolute_error: 0.4077\n",
      "Epoch 20/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3277 - root_mean_squared_error: 0.5724 - mean_absolute_error: 0.4008 - val_loss: 0.3740 - val_root_mean_squared_error: 0.6116 - val_mean_absolute_error: 0.4089\n",
      "Epoch 21/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3245 - root_mean_squared_error: 0.5697 - mean_absolute_error: 0.3984 - val_loss: 0.3603 - val_root_mean_squared_error: 0.6003 - val_mean_absolute_error: 0.4141\n",
      "Epoch 22/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3234 - root_mean_squared_error: 0.5686 - mean_absolute_error: 0.3980 - val_loss: 0.3590 - val_root_mean_squared_error: 0.5992 - val_mean_absolute_error: 0.4033\n",
      "Epoch 23/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3198 - root_mean_squared_error: 0.5655 - mean_absolute_error: 0.3954 - val_loss: 0.3583 - val_root_mean_squared_error: 0.5986 - val_mean_absolute_error: 0.4014\n",
      "Epoch 24/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3245 - root_mean_squared_error: 0.5696 - mean_absolute_error: 0.3981 - val_loss: 0.3634 - val_root_mean_squared_error: 0.6028 - val_mean_absolute_error: 0.4100\n",
      "Epoch 25/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3315 - root_mean_squared_error: 0.5758 - mean_absolute_error: 0.3984 - val_loss: 0.3582 - val_root_mean_squared_error: 0.5985 - val_mean_absolute_error: 0.4022\n",
      "Epoch 26/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3192 - root_mean_squared_error: 0.5650 - mean_absolute_error: 0.3955 - val_loss: 0.3699 - val_root_mean_squared_error: 0.6082 - val_mean_absolute_error: 0.4053\n",
      "Epoch 27/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3183 - root_mean_squared_error: 0.5641 - mean_absolute_error: 0.3946 - val_loss: 0.3670 - val_root_mean_squared_error: 0.6058 - val_mean_absolute_error: 0.4045\n",
      "Epoch 28/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3173 - root_mean_squared_error: 0.5633 - mean_absolute_error: 0.3941 - val_loss: 0.3772 - val_root_mean_squared_error: 0.6142 - val_mean_absolute_error: 0.4055\n",
      "Epoch 29/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3182 - root_mean_squared_error: 0.5641 - mean_absolute_error: 0.3942 - val_loss: 0.3634 - val_root_mean_squared_error: 0.6028 - val_mean_absolute_error: 0.4124\n",
      "Epoch 30/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3175 - root_mean_squared_error: 0.5635 - mean_absolute_error: 0.3932 - val_loss: 0.3669 - val_root_mean_squared_error: 0.6057 - val_mean_absolute_error: 0.4035\n",
      "Epoch 31/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3206 - root_mean_squared_error: 0.5662 - mean_absolute_error: 0.3953 - val_loss: 0.3626 - val_root_mean_squared_error: 0.6021 - val_mean_absolute_error: 0.4003\n",
      "Epoch 32/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3154 - root_mean_squared_error: 0.5616 - mean_absolute_error: 0.3932 - val_loss: 0.3589 - val_root_mean_squared_error: 0.5991 - val_mean_absolute_error: 0.4045\n",
      "Epoch 33/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3225 - root_mean_squared_error: 0.5679 - mean_absolute_error: 0.3942 - val_loss: 0.3616 - val_root_mean_squared_error: 0.6013 - val_mean_absolute_error: 0.3971\n",
      "Epoch 34/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3152 - root_mean_squared_error: 0.5614 - mean_absolute_error: 0.3921 - val_loss: 0.3685 - val_root_mean_squared_error: 0.6071 - val_mean_absolute_error: 0.4105\n",
      "Epoch 35/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3148 - root_mean_squared_error: 0.5610 - mean_absolute_error: 0.3917 - val_loss: 0.3732 - val_root_mean_squared_error: 0.6109 - val_mean_absolute_error: 0.4286\n",
      "Epoch 36/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3140 - root_mean_squared_error: 0.5604 - mean_absolute_error: 0.3915 - val_loss: 0.3560 - val_root_mean_squared_error: 0.5966 - val_mean_absolute_error: 0.4025\n",
      "Epoch 37/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3129 - root_mean_squared_error: 0.5593 - mean_absolute_error: 0.3909 - val_loss: 0.3541 - val_root_mean_squared_error: 0.5950 - val_mean_absolute_error: 0.3997\n",
      "Epoch 38/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3133 - root_mean_squared_error: 0.5597 - mean_absolute_error: 0.3910 - val_loss: 0.4022 - val_root_mean_squared_error: 0.6342 - val_mean_absolute_error: 0.4085\n",
      "Epoch 39/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3161 - root_mean_squared_error: 0.5622 - mean_absolute_error: 0.3916 - val_loss: 0.3669 - val_root_mean_squared_error: 0.6057 - val_mean_absolute_error: 0.4057\n",
      "Epoch 40/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3135 - root_mean_squared_error: 0.5599 - mean_absolute_error: 0.3900 - val_loss: 0.3699 - val_root_mean_squared_error: 0.6082 - val_mean_absolute_error: 0.4105\n",
      "Epoch 41/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3124 - root_mean_squared_error: 0.5589 - mean_absolute_error: 0.3905 - val_loss: 0.3579 - val_root_mean_squared_error: 0.5982 - val_mean_absolute_error: 0.3950\n",
      "Epoch 42/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3151 - root_mean_squared_error: 0.5614 - mean_absolute_error: 0.3912 - val_loss: 0.3566 - val_root_mean_squared_error: 0.5972 - val_mean_absolute_error: 0.3959\n",
      "Epoch 43/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3125 - root_mean_squared_error: 0.5590 - mean_absolute_error: 0.3900 - val_loss: 0.3539 - val_root_mean_squared_error: 0.5949 - val_mean_absolute_error: 0.4037\n",
      "Epoch 44/50\n",
      "363/363 [==============================] - 0s 986us/step - loss: 0.3110 - root_mean_squared_error: 0.5577 - mean_absolute_error: 0.3899 - val_loss: 0.3581 - val_root_mean_squared_error: 0.5984 - val_mean_absolute_error: 0.4023\n",
      "Epoch 45/50\n",
      "363/363 [==============================] - 0s 959us/step - loss: 0.3116 - root_mean_squared_error: 0.5582 - mean_absolute_error: 0.3891 - val_loss: 0.3521 - val_root_mean_squared_error: 0.5934 - val_mean_absolute_error: 0.3984\n",
      "Epoch 46/50\n",
      "363/363 [==============================] - 0s 970us/step - loss: 0.3101 - root_mean_squared_error: 0.5569 - mean_absolute_error: 0.3889 - val_loss: 0.3561 - val_root_mean_squared_error: 0.5967 - val_mean_absolute_error: 0.3969\n",
      "Epoch 47/50\n",
      "363/363 [==============================] - 0s 999us/step - loss: 0.3097 - root_mean_squared_error: 0.5565 - mean_absolute_error: 0.3881 - val_loss: 0.3528 - val_root_mean_squared_error: 0.5940 - val_mean_absolute_error: 0.3935\n",
      "Epoch 48/50\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3122 - root_mean_squared_error: 0.5587 - mean_absolute_error: 0.3904 - val_loss: 0.3594 - val_root_mean_squared_error: 0.5995 - val_mean_absolute_error: 0.3980\n",
      "Epoch 49/50\n",
      "363/363 [==============================] - 0s 985us/step - loss: 0.3125 - root_mean_squared_error: 0.5590 - mean_absolute_error: 0.3902 - val_loss: 0.3561 - val_root_mean_squared_error: 0.5968 - val_mean_absolute_error: 0.3986\n",
      "Epoch 50/50\n",
      "363/363 [==============================] - 0s 976us/step - loss: 0.3151 - root_mean_squared_error: 0.5614 - mean_absolute_error: 0.3914 - val_loss: 0.3577 - val_root_mean_squared_error: 0.5981 - val_mean_absolute_error: 0.4010\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPara lanzarlo desde el jupyter notebook\\n%load_ext tensorboard\\n%tensorboard --logdir=./my_logs --port=6006\\n\\nPara lanzarlo desde el terminal, hay que estar en la carpeta de los logs\\ntensorboard --logdir=./my_logs --port=6006\\n\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Para lanzarlo desde el jupyter notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006\n",
    "\n",
    "Para lanzarlo desde el terminal, hay que estar en la carpeta de los logs\n",
    "tensorboard --logdir=./my_logs --port=6006\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BootCamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "167a7833a0358ac30a26ad970c5914014f41a5348f3dc652232a762d6e3283fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
